{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/\n",
    "\n",
    "RNN과 LSTM 잘 정리돼있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras를 이용한 RNN 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN은 히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는(directed cycle) 인공신경망의 한 종류.\n",
    "- 음성, 문자 등 순차적으로 등장하는 데이터 처리에 적합한 모델\n",
    "- 역전파(backpropagation)를 수행해 parameter값들을 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 문제점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 관련 정보와 그 정보를 사용하는 지점 사이 거리가 멀 경우 역전파시 그래디언트가 점차 줄어 학습능력이 크게 저하되는 것으로 알려져 있습니다. 이를 vanishing gradient problem 이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 등장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문제를 극복하기 위해서 고안된 것이 바로 LSTM입니다. LSTM은 RNN의 히든 state에 cell-state를 추가한 구조\n",
    "cell state는 일종의 컨베이어 벨트 역할을 합니다. 덕분에 state가 꽤 오래 경과하더라도 그래디언트가 비교적 전파가 잘 되게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2idx = {'c4':0, 'd4':1, 'e4':2, 'f4':3, 'g4':4, 'a4':5, 'b4':6,\n",
    "            'c8':7, 'd8':8, 'e8':9, 'f8':10, 'g8':11, 'a8':12, 'b8':13}\n",
    "\n",
    "idx2code = {0:'c4', 1:'d4', 2:'e4', 3:'f4', 4:'g4', 5:'a4', 6:'b4',\n",
    "            7:'c8', 8:'d8', 9:'e8', 10:'f8', 11:'g8', 12:'a8', 13:'b8'}\n",
    "\n",
    "seq = ['g8', 'e8', 'e4', 'f8', 'd8', 'd4', 'c8', 'd8', 'e8', 'f8', 'g8', 'g8', 'g4',\n",
    "       'g8', 'e8', 'e8', 'e8', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8', 'e4',\n",
    "       'd8', 'd8', 'd8', 'd8', 'd8', 'e8', 'f4', 'e8', 'e8', 'e8', 'e8', 'e8', 'f8', 'g4',\n",
    "       'g8', 'e8', 'e4', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8', 'e4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g8']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g8', 'e8', 'e4', 'f8']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0:0+3+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e8', 'e4', 'f8', 'd8']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[1:1+3+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트 자체를 넣으면 안되고..window~\n",
    "# 1234를 잘라서 5번째 예측, 2345 잘라서 6번째 예측\n",
    "\n",
    "def seq2dataset(seq, window_size):##어떤 역할을 하는지 생각해보기\n",
    "    dataset = []\n",
    "    for i in range(len(seq)-window_size): # len(seq)-window_size : 51 --- 0부터 50까지 돈다\n",
    "        subset = seq[i:(i+window_size+1)] # len(seq) : 54, window_size : 3 / # seq 에는 음악코드가 나와있음\n",
    "        dataset.append([code2idx[item] for item in subset])\n",
    "        # code2idx : 딕셔너리 {key(음악코드) : value(숫자)}\n",
    "        # value(숫자)를 dataset 리스트에 담는다\n",
    "    return np.array(dataset) # dataset 리스트를 배열형태로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 4)\n",
      "---------------------\n",
      "[[11  9  2 10]\n",
      " [ 9  2 10  8]\n",
      " [ 2 10  8  1]\n",
      " [10  8  1  7]\n",
      " [ 8  1  7  8]\n",
      " [ 1  7  8  9]\n",
      " [ 7  8  9 10]\n",
      " [ 8  9 10 11]\n",
      " [ 9 10 11 11]\n",
      " [10 11 11  4]\n",
      " [11 11  4 11]\n",
      " [11  4 11  9]\n",
      " [ 4 11  9  9]\n",
      " [11  9  9  9]\n",
      " [ 9  9  9 10]\n",
      " [ 9  9 10  8]\n",
      " [ 9 10  8  1]\n",
      " [10  8  1  7]\n",
      " [ 8  1  7  9]\n",
      " [ 1  7  9 11]\n",
      " [ 7  9 11 11]\n",
      " [ 9 11 11  9]\n",
      " [11 11  9  9]\n",
      " [11  9  9  2]\n",
      " [ 9  9  2  8]\n",
      " [ 9  2  8  8]\n",
      " [ 2  8  8  8]\n",
      " [ 8  8  8  8]\n",
      " [ 8  8  8  8]\n",
      " [ 8  8  8  9]\n",
      " [ 8  8  9  3]\n",
      " [ 8  9  3  9]\n",
      " [ 9  3  9  9]\n",
      " [ 3  9  9  9]\n",
      " [ 9  9  9  9]\n",
      " [ 9  9  9  9]\n",
      " [ 9  9  9 10]\n",
      " [ 9  9 10  4]\n",
      " [ 9 10  4 11]\n",
      " [10  4 11  9]\n",
      " [ 4 11  9  2]\n",
      " [11  9  2 10]\n",
      " [ 9  2 10  8]\n",
      " [ 2 10  8  1]\n",
      " [10  8  1  7]\n",
      " [ 8  1  7  9]\n",
      " [ 1  7  9 11]\n",
      " [ 7  9 11 11]\n",
      " [ 9 11 11  9]\n",
      " [11 11  9  9]\n",
      " [11  9  9  2]]\n"
     ]
    }
   ],
   "source": [
    "dataset = seq2dataset(seq, window_size = 3) \n",
    "#윈도우사이즈 n : n개의 숫자를 이용해서 다음 숫자를 예측\n",
    "\n",
    "print(dataset.shape)\n",
    "print(\"---------------------\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)\n",
    "# ',' 없잖아~!! 리스트 아니징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = seq2dataset(seq, window_size = 4) \n",
    "# #윈도우사이즈 n : n개의 숫자를 이용해서 다음 숫자를 예측\n",
    "\n",
    "# print(dataset.shape)\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  9,  2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:1, 0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  9,  2, 10]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:1, 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  8,  1,  7,  8,  9, 10, 11, 11,  4, 11,  9,  9,  9, 10,  8,  1,\n",
       "        7,  9, 11, 11,  9,  9,  2,  8,  8,  8,  8,  8,  9,  3,  9,  9,  9,\n",
       "        9,  9, 10,  4, 11,  9,  2, 10,  8,  1,  7,  9, 11, 11,  9,  9,  2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential # 케라스 모델의 기본\n",
    "from keras.layers import Conv2D # 2D형태로 하겠다\n",
    "from keras.layers import MaxPooling2D # 2D를 자르고 붙일 때~ 큰 값을 붙이겠다. c.f) Pooling 개념.\n",
    "from keras.layers import Flatten # CNN (여러 층)으로 쌓은 것을 한층으로 변환\n",
    "from keras.layers import Dense,LSTM # RNN 할 때 씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "# np_utils.to_categorical :::: Converts a class vector (integers) to binary class matrix.\n",
    "# 정답 갯수만큼 길이가 생성되고, 해당하는 값만 1로 표현해준다.\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=dataset[:,0:4] # 주의 --- '0:4' 의미는 Dataframe의 속성 갯수가 아니라 window size이다. 현재, 속성은 1개임.\n",
    "y_train=dataset[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx_value=13\n",
    "x_train = x_train / float(max_idx_value)\n",
    "# Scale 하는 것 같네. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84615385, 0.69230769, 0.15384615, 0.76923077],\n",
       "       [0.69230769, 0.15384615, 0.76923077, 0.61538462],\n",
       "       [0.15384615, 0.76923077, 0.61538462, 0.07692308],\n",
       "       [0.76923077, 0.61538462, 0.07692308, 0.53846154],\n",
       "       [0.61538462, 0.07692308, 0.53846154, 0.61538462],\n",
       "       [0.07692308, 0.53846154, 0.61538462, 0.69230769],\n",
       "       [0.53846154, 0.61538462, 0.69230769, 0.76923077],\n",
       "       [0.61538462, 0.69230769, 0.76923077, 0.84615385],\n",
       "       [0.69230769, 0.76923077, 0.84615385, 0.84615385],\n",
       "       [0.76923077, 0.84615385, 0.84615385, 0.30769231],\n",
       "       [0.84615385, 0.84615385, 0.30769231, 0.84615385],\n",
       "       [0.84615385, 0.30769231, 0.84615385, 0.69230769],\n",
       "       [0.30769231, 0.84615385, 0.69230769, 0.69230769],\n",
       "       [0.84615385, 0.69230769, 0.69230769, 0.69230769],\n",
       "       [0.69230769, 0.69230769, 0.69230769, 0.76923077],\n",
       "       [0.69230769, 0.69230769, 0.76923077, 0.61538462],\n",
       "       [0.69230769, 0.76923077, 0.61538462, 0.07692308],\n",
       "       [0.76923077, 0.61538462, 0.07692308, 0.53846154],\n",
       "       [0.61538462, 0.07692308, 0.53846154, 0.69230769],\n",
       "       [0.07692308, 0.53846154, 0.69230769, 0.84615385],\n",
       "       [0.53846154, 0.69230769, 0.84615385, 0.84615385],\n",
       "       [0.69230769, 0.84615385, 0.84615385, 0.69230769],\n",
       "       [0.84615385, 0.84615385, 0.69230769, 0.69230769],\n",
       "       [0.84615385, 0.69230769, 0.69230769, 0.15384615],\n",
       "       [0.69230769, 0.69230769, 0.15384615, 0.61538462],\n",
       "       [0.69230769, 0.15384615, 0.61538462, 0.61538462],\n",
       "       [0.15384615, 0.61538462, 0.61538462, 0.61538462],\n",
       "       [0.61538462, 0.61538462, 0.61538462, 0.61538462],\n",
       "       [0.61538462, 0.61538462, 0.61538462, 0.61538462],\n",
       "       [0.61538462, 0.61538462, 0.61538462, 0.69230769],\n",
       "       [0.61538462, 0.61538462, 0.69230769, 0.23076923],\n",
       "       [0.61538462, 0.69230769, 0.23076923, 0.69230769],\n",
       "       [0.69230769, 0.23076923, 0.69230769, 0.69230769],\n",
       "       [0.23076923, 0.69230769, 0.69230769, 0.69230769],\n",
       "       [0.69230769, 0.69230769, 0.69230769, 0.69230769],\n",
       "       [0.69230769, 0.69230769, 0.69230769, 0.69230769],\n",
       "       [0.69230769, 0.69230769, 0.69230769, 0.76923077],\n",
       "       [0.69230769, 0.69230769, 0.76923077, 0.30769231],\n",
       "       [0.69230769, 0.76923077, 0.30769231, 0.84615385],\n",
       "       [0.76923077, 0.30769231, 0.84615385, 0.69230769],\n",
       "       [0.30769231, 0.84615385, 0.69230769, 0.15384615],\n",
       "       [0.84615385, 0.69230769, 0.15384615, 0.76923077],\n",
       "       [0.69230769, 0.15384615, 0.76923077, 0.61538462],\n",
       "       [0.15384615, 0.76923077, 0.61538462, 0.07692308],\n",
       "       [0.76923077, 0.61538462, 0.07692308, 0.53846154],\n",
       "       [0.61538462, 0.07692308, 0.53846154, 0.69230769],\n",
       "       [0.07692308, 0.53846154, 0.69230769, 0.84615385],\n",
       "       [0.53846154, 0.69230769, 0.84615385, 0.84615385],\n",
       "       [0.69230769, 0.84615385, 0.84615385, 0.69230769],\n",
       "       [0.84615385, 0.84615385, 0.69230769, 0.69230769],\n",
       "       [0.84615385, 0.69230769, 0.69230769, 0.15384615]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  8,  1,  7,  8,  9, 10, 11, 11,  4, 11,  9,  9,  9, 10,  8,  1,\n",
       "        7,  9, 11, 11,  9,  9,  2,  8,  8,  8,  8,  8,  9,  3,  9,  9,  9,\n",
       "        9,  9, 10,  4, 11,  9,  2, 10,  8,  1,  7,  9, 11, 11,  9,  9,  2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=np_utils.to_categorical(y_train)\n",
    "# np_utils.to_categorical :::: Converts a class vector (integers) to binary class matrix.\n",
    "# 정답 갯수만큼 길이가 생성되고, 해당하는 값만 1로 표현해준다.\n",
    "# 정답 갯수만큼 생기는 건 아닌가보네. '0'이 생겨있음.\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0] # 10 이란 뜻!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0] # y_train (즉 정답(label) 51개 있단거지!)\n",
    "y_train.shape[1] # 총 12개의 종류가 있다는 것(이건 확실하지 않음. 0이 끼어져 있음. 종류+1개 만큼 생기는건가?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoding vector size is 12\n"
     ]
    }
   ],
   "source": [
    "one_hot_vec_size = y_train.shape[1]\n",
    "print('one hot encoding vector size is',one_hot_vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 이력 클래스 정의 : 손실을 기록하기 위해 함수 생성\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def init(self):\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = LossHistory() \n",
    "history.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ※입출력을 모두 연결해주는 Dense 레이어\n",
    "    \n",
    "    - Dense 레이어는 입력과 출력을 모두 연결해줍니다. 예를 들어 입력 뉴런이 4개, 출력 뉴런이 8개있다면 총 연결선은 32개(4*8=32) 입니다. \n",
    "    각 연결선에는 가중치(weight)를 포함하고 있는데, 이 가중치가 나타내는 의미는 연결강도라고 보시면 됩니다. \n",
    "    연결선이 32개이므로 가중치도 32개입니다.\n",
    "    \n",
    "    - 가중치가 높을수록 해당 입력 뉴런이 출력 뉴런에 미치는 영향이 크고, 낮을수록 미치는 영향이 적다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dense(8, input_dim=4, init='uniform', activation='relu'))\n",
    "\n",
    " - 첫번째 인자 : 출력 뉴런의 수를 설정합니다.\n",
    " - input_dim : 입력 뉴런의 수를 설정합니다.\n",
    " - init : 가중치 초기화 방법 설정합니다.\n",
    " - 'uniform’ : 균일 분포\n",
    " - ‘normal’ : 가우시안 분포\n",
    " - activation : 활성화 함수 설정합니다.\n",
    " - ‘linear’ : 디폴트 값, 입력뉴런과 가중치로 계산된 결과값이 그대로 출력으로 나옵니다.\n",
    " - ‘relu’ : rectifier 함수, 은익층에 주로 쓰입니다.\n",
    " - ‘sigmoid’ : 시그모이드 함수, 이진 분류 문제에서 출력층에 주로 쓰입니다.\n",
    " - ‘softmax’ : 소프트맥스 함수, 다중 클래스 분류 문제에서 출력층에 주로 쓰입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense 레이어는 입력 뉴런 수에 상관없이 출력 뉴런 수를 자유롭게 설정할 수 있기 때문에 출력층으로 많이 사용됩니다. 이진 분류문제에서는 0과 1을 나타내는 출력 뉴런이 하나만 있으면 되기 때문에 아래 코드처럼 출력 뉴런이 1개이고, 입력 뉴런과 가중치를 계산한 값을 0에서 1사이로 표현할 수 있는 활성화 함수인 sigmoid을 사용합니다.\n",
    "    \n",
    "Dense(1, input_dim=3, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  모델 생성 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "model.add(Dense(128,input_dim=4,activation='relu'))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(one_hot_vec_size,activation='softmax')) #출력 클래스가 몇개인가? 12개(one_hot_vec_size == 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense 레이어는 보통 출력층 이전의 은닉층으로도 많이 쓰이고, 영상이 아닌 수치자료 입력 시에는 입력층으로도 많이 쓰입니다. 이 때 활성화 함수로 ‘relu’가 주로 사용됩니다. ‘relu’는 학습과정에서 역전파 시에 좋은 성능이 나는 것으로 알려져 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train \n",
    "# 정답을 나열해두고, 해당하는 것에만 1로 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# 바이너리가 아니라, categorical : 정답이 여러개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 0s - loss: 2.4761 - acc: 0.1373\n",
      "Epoch 2/200\n",
      " - 0s - loss: 2.3409 - acc: 0.2157\n",
      "Epoch 3/200\n",
      " - 0s - loss: 2.2446 - acc: 0.3529\n",
      "Epoch 4/200\n",
      " - 0s - loss: 2.1583 - acc: 0.3333\n",
      "Epoch 5/200\n",
      " - 0s - loss: 2.0760 - acc: 0.3333\n",
      "Epoch 6/200\n",
      " - 0s - loss: 2.0006 - acc: 0.3333\n",
      "Epoch 7/200\n",
      " - 0s - loss: 1.9387 - acc: 0.3333\n",
      "Epoch 8/200\n",
      " - 0s - loss: 1.8927 - acc: 0.3333\n",
      "Epoch 9/200\n",
      " - 0s - loss: 1.8592 - acc: 0.3333\n",
      "Epoch 10/200\n",
      " - 0s - loss: 1.8313 - acc: 0.3333\n",
      "Epoch 11/200\n",
      " - 0s - loss: 1.8042 - acc: 0.3333\n",
      "Epoch 12/200\n",
      " - 0s - loss: 1.7764 - acc: 0.3725\n",
      "Epoch 13/200\n",
      " - 0s - loss: 1.7469 - acc: 0.3725\n",
      "Epoch 14/200\n",
      " - 0s - loss: 1.7158 - acc: 0.3922\n",
      "Epoch 15/200\n",
      " - 0s - loss: 1.6857 - acc: 0.3922\n",
      "Epoch 16/200\n",
      " - 0s - loss: 1.6543 - acc: 0.3922\n",
      "Epoch 17/200\n",
      " - 0s - loss: 1.6199 - acc: 0.3922\n",
      "Epoch 18/200\n",
      " - 0s - loss: 1.5869 - acc: 0.3922\n",
      "Epoch 19/200\n",
      " - 0s - loss: 1.5580 - acc: 0.3922\n",
      "Epoch 20/200\n",
      " - 0s - loss: 1.5298 - acc: 0.4314\n",
      "Epoch 21/200\n",
      " - 0s - loss: 1.5004 - acc: 0.4510\n",
      "Epoch 22/200\n",
      " - 0s - loss: 1.4718 - acc: 0.4510\n",
      "Epoch 23/200\n",
      " - 0s - loss: 1.4454 - acc: 0.4510\n",
      "Epoch 24/200\n",
      " - 0s - loss: 1.4208 - acc: 0.5098\n",
      "Epoch 25/200\n",
      " - 0s - loss: 1.3975 - acc: 0.5294\n",
      "Epoch 26/200\n",
      " - 0s - loss: 1.3755 - acc: 0.5098\n",
      "Epoch 27/200\n",
      " - 0s - loss: 1.3543 - acc: 0.5098\n",
      "Epoch 28/200\n",
      " - 0s - loss: 1.3349 - acc: 0.5098\n",
      "Epoch 29/200\n",
      " - 0s - loss: 1.3173 - acc: 0.5098\n",
      "Epoch 30/200\n",
      " - 0s - loss: 1.2999 - acc: 0.5098\n",
      "Epoch 31/200\n",
      " - 0s - loss: 1.2842 - acc: 0.5294\n",
      "Epoch 32/200\n",
      " - 0s - loss: 1.2690 - acc: 0.5490\n",
      "Epoch 33/200\n",
      " - 0s - loss: 1.2547 - acc: 0.5490\n",
      "Epoch 34/200\n",
      " - 0s - loss: 1.2414 - acc: 0.5490\n",
      "Epoch 35/200\n",
      " - 0s - loss: 1.2286 - acc: 0.5490\n",
      "Epoch 36/200\n",
      " - 0s - loss: 1.2163 - acc: 0.5294\n",
      "Epoch 37/200\n",
      " - 0s - loss: 1.2046 - acc: 0.5294\n",
      "Epoch 38/200\n",
      " - 0s - loss: 1.1940 - acc: 0.5294\n",
      "Epoch 39/200\n",
      " - 0s - loss: 1.1829 - acc: 0.5098\n",
      "Epoch 40/200\n",
      " - 0s - loss: 1.1725 - acc: 0.5098\n",
      "Epoch 41/200\n",
      " - 0s - loss: 1.1629 - acc: 0.5098\n",
      "Epoch 42/200\n",
      " - 0s - loss: 1.1534 - acc: 0.5098\n",
      "Epoch 43/200\n",
      " - 0s - loss: 1.1440 - acc: 0.5098\n",
      "Epoch 44/200\n",
      " - 0s - loss: 1.1351 - acc: 0.5098\n",
      "Epoch 45/200\n",
      " - 0s - loss: 1.1262 - acc: 0.5098\n",
      "Epoch 46/200\n",
      " - 0s - loss: 1.1179 - acc: 0.5098\n",
      "Epoch 47/200\n",
      " - 0s - loss: 1.1094 - acc: 0.5098\n",
      "Epoch 48/200\n",
      " - 0s - loss: 1.1013 - acc: 0.5098\n",
      "Epoch 49/200\n",
      " - 0s - loss: 1.0934 - acc: 0.5098\n",
      "Epoch 50/200\n",
      " - 0s - loss: 1.0859 - acc: 0.5098\n",
      "Epoch 51/200\n",
      " - 0s - loss: 1.0787 - acc: 0.5098\n",
      "Epoch 52/200\n",
      " - 0s - loss: 1.0709 - acc: 0.5098\n",
      "Epoch 53/200\n",
      " - 0s - loss: 1.0640 - acc: 0.5098\n",
      "Epoch 54/200\n",
      " - 0s - loss: 1.0575 - acc: 0.5098\n",
      "Epoch 55/200\n",
      " - 0s - loss: 1.0504 - acc: 0.5098\n",
      "Epoch 56/200\n",
      " - 0s - loss: 1.0423 - acc: 0.5098\n",
      "Epoch 57/200\n",
      " - 0s - loss: 1.0357 - acc: 0.5098\n",
      "Epoch 58/200\n",
      " - 0s - loss: 1.0291 - acc: 0.5098\n",
      "Epoch 59/200\n",
      " - 0s - loss: 1.0227 - acc: 0.5490\n",
      "Epoch 60/200\n",
      " - 0s - loss: 1.0156 - acc: 0.5490\n",
      "Epoch 61/200\n",
      " - 0s - loss: 1.0086 - acc: 0.5490\n",
      "Epoch 62/200\n",
      " - 0s - loss: 1.0020 - acc: 0.5490\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.9949 - acc: 0.5490\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.9881 - acc: 0.5490\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.9816 - acc: 0.5490\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.9750 - acc: 0.5490\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.9679 - acc: 0.5686\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.9622 - acc: 0.5686\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.9554 - acc: 0.5882\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.9487 - acc: 0.5882\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.9421 - acc: 0.5882\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.9355 - acc: 0.5882\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.9287 - acc: 0.5882\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.9231 - acc: 0.5882\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.9164 - acc: 0.5882\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.9098 - acc: 0.5882\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.9042 - acc: 0.5882\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.8975 - acc: 0.6275\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.8917 - acc: 0.6078\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.8852 - acc: 0.6275\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.8787 - acc: 0.6275\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.8733 - acc: 0.6275\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.8663 - acc: 0.6471\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.8599 - acc: 0.6667\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.8533 - acc: 0.6667\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.8477 - acc: 0.6667\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.8412 - acc: 0.6667\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.8354 - acc: 0.6667\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.8292 - acc: 0.6667\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.8230 - acc: 0.6667\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.8168 - acc: 0.6667\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.8109 - acc: 0.6667\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.8045 - acc: 0.6863\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.7979 - acc: 0.6863\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.7925 - acc: 0.6863\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.7864 - acc: 0.7059\n",
      "Epoch 97/200\n",
      " - 0s - loss: 0.7803 - acc: 0.7059\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.7748 - acc: 0.7255\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.7684 - acc: 0.7255\n",
      "Epoch 100/200\n",
      " - 0s - loss: 0.7627 - acc: 0.7255\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.7573 - acc: 0.7255\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.7512 - acc: 0.7255\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.7456 - acc: 0.7255\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.7399 - acc: 0.7255\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.7339 - acc: 0.7255\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.7291 - acc: 0.7255\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.7237 - acc: 0.7255\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.7178 - acc: 0.7255\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.7127 - acc: 0.7255\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.7075 - acc: 0.7255\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.7019 - acc: 0.7255\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.6969 - acc: 0.7255\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.6914 - acc: 0.7255\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.6863 - acc: 0.7255\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.6811 - acc: 0.7451\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.6757 - acc: 0.7451\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.6706 - acc: 0.7647\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.6655 - acc: 0.7647\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.6603 - acc: 0.7647\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.6559 - acc: 0.7843\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.6511 - acc: 0.7647\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.6457 - acc: 0.7843\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.6407 - acc: 0.7843\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.6363 - acc: 0.7843\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.6313 - acc: 0.7843\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.6265 - acc: 0.7843\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.6218 - acc: 0.7843\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.6174 - acc: 0.7843\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.6128 - acc: 0.7843\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.6085 - acc: 0.7843\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.6036 - acc: 0.7843\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.5992 - acc: 0.7843\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.5942 - acc: 0.7843\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.5905 - acc: 0.7843\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.5858 - acc: 0.7843\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.5811 - acc: 0.7843\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.5767 - acc: 0.7843\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.5725 - acc: 0.7843\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.5686 - acc: 0.7843\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.5641 - acc: 0.7843\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.5596 - acc: 0.8039\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.5555 - acc: 0.8039\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.5516 - acc: 0.8039\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.5474 - acc: 0.8039\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.5437 - acc: 0.8235\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.5390 - acc: 0.8235\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.5353 - acc: 0.8235\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.5310 - acc: 0.8235\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.5267 - acc: 0.8235\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.5230 - acc: 0.8235\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.5192 - acc: 0.8235\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.5149 - acc: 0.8235\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.5109 - acc: 0.8235\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.5073 - acc: 0.8235\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.5028 - acc: 0.8627\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.4997 - acc: 0.8627\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.4962 - acc: 0.8627\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.4916 - acc: 0.8627\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.4882 - acc: 0.8627\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.4844 - acc: 0.8627\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.4801 - acc: 0.8824\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.4765 - acc: 0.8824\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.4731 - acc: 0.8824\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.4696 - acc: 0.8824\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.4659 - acc: 0.8824\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.4623 - acc: 0.8824\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.4583 - acc: 0.8824\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.4550 - acc: 0.8824\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.4522 - acc: 0.8824\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.4485 - acc: 0.8824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200\n",
      " - 0s - loss: 0.4443 - acc: 0.8824\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.4410 - acc: 0.8824\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.4371 - acc: 0.8824\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.4342 - acc: 0.8824\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.4308 - acc: 0.8824\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.4274 - acc: 0.8824\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.4239 - acc: 0.8824\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.4203 - acc: 0.8824\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.4173 - acc: 0.8824\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.4141 - acc: 0.8824\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.4102 - acc: 0.8824\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.4072 - acc: 0.8824\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.4041 - acc: 0.8824\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.4003 - acc: 0.8824\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.3971 - acc: 0.8824\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.3939 - acc: 0.8824\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.3903 - acc: 0.8824\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.3876 - acc: 0.8824\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.3842 - acc: 0.8824\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.3814 - acc: 0.8824\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.3782 - acc: 0.8824\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.3745 - acc: 0.8824\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.3712 - acc: 0.8824\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.3686 - acc: 0.8824\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.3653 - acc: 0.8824\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.3619 - acc: 0.8824\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.3593 - acc: 0.8824\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.3556 - acc: 0.8824\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.3532 - acc: 0.8824\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.3494 - acc: 0.9020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20363540208>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epoch=200\n",
    "model.fit(x_train,y_train,epochs=200,batch_size=10,verbose=2,shuffle=False,callbacks=[history]) # callbaks =[history] 의미!?\n",
    "#모델 실행하겠다. epochs, batch_size 두 파라미터를 많이 사용한다. callbacks : 이력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 665us/step\n",
      "acc: 90.20%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_train, y_train)\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_count = 50 # 최대 예측 개수 정의\n",
    "\n",
    "\n",
    "seq_out = ['g8', 'e8', 'e4', 'f8']\n",
    "pred_out = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one step prediction :  ['g8', 'e8', 'e4', 'f8', 'f8', 'd8', 'd4', 'c8', 'd8', 'e8', 'g8', 'g8', 'g8', 'g4', 'g8', 'e8', 'e8', 'e8', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8', 'e4', 'd8', 'd8', 'e8', 'e8', 'e8', 'e8', 'e4', 'e8', 'e8', 'e8', 'e8', 'e8', 'f8', 'g4', 'g8', 'e8', 'e4', 'f8', 'd8', 'd4', 'c8', 'e8', 'g8', 'g8', 'e8', 'e8']\n"
     ]
    }
   ],
   "source": [
    "for i in range(pred_count):\n",
    "    idx = np.argmax(pred_out[i]) # one-hot 인코딩을 인덱스 값으로 변환\n",
    "    seq_out.append(idx2code[idx]) # seq_out는 최종 악보이므로 인덱스 값을 코드로 변환하여 저장\n",
    "    \n",
    "print(\"one step prediction : \", seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full song prediction :  ['g8', 'e8', 'e4', 'f8', 'f8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8', 'e8']\n"
     ]
    }
   ],
   "source": [
    "# 곡 전체 예측\n",
    "\n",
    "seq_in = ['g8', 'e8', 'e4', 'f8']\n",
    "seq_out = seq_in\n",
    "seq_in = [code2idx[it] / float(max_idx_value) for it in seq_in] # 코드를 인덱스값으로 변환\n",
    "\n",
    "for i in range(pred_count):\n",
    "    sample_in = np.array(seq_in)\n",
    "    sample_in = np.reshape(sample_in,(1,4)) #batch_size,속성수\n",
    "    pred_out = model.predict(sample_in) # X_test 넣어야지~\n",
    "    idx = np.argmax(pred_out)\n",
    "    seq_out.append(idx2code[idx])\n",
    "    seq_in.append(idx / float(max_idx_value))\n",
    "    seq_in.pop(0)\n",
    "\n",
    "print(\"full song prediction : \", seq_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
